---
sidebar: sidebar 
permalink: infra/ai-dgx-superpod-nva1175-deploy.html 
keywords: NetApp AI, AI, Artificial Intelligence, ML, Machine Learning, NVIDIA, NVIDIA AI Enterprise, NVIDIA SuperPOD, NVIDIA DGX 
summary: NVIDIA DGX SuperPOD con NetApp AFF A90 
---
= Sistemi di storage NetApp AFF A90 con NVIDIA DGX SuperPOD
:allow-uri-read: 




== Dispiegamento NVA

[role="lead"]
I sistemi di storage NVIDIA DGX SuperPOD con NetApp AFF A90 combinano le prestazioni di elaborazione di livello mondiale dei sistemi NVIDIA DGX con i sistemi di storage NetApp connessi al cloud per abilitare flussi di lavoro basati sui dati per l'apprendimento automatico (ML), l'intelligenza artificiale (AI) e il calcolo tecnico ad alte prestazioni (HPC).  Questo documento descrive i dettagli di configurazione e distribuzione per l'integrazione dei sistemi di storage AFF A90 nell'architettura DGX SuperPOD.

image:nvidialogo.png["Logo nvidia"]

David Arnette, NetApp



== Riepilogo del programma

NVIDIA DGX SuperPOD™ offre una soluzione di data center AI chiavi in ​​mano per le organizzazioni, offrendo senza soluzione di continuità elaborazione, strumenti software, competenze e innovazione continua di livello mondiale.  DGX SuperPOD offre ai clienti tutto ciò di cui hanno bisogno per distribuire carichi di lavoro AI/ML e HPC con tempi di configurazione minimi e massima produttività.  La figura 1 mostra i componenti di alto livello di DGX SuperPOD.

Figura 1) NVIDIA DGX SuperPOD con sistemi di storage NetApp AFF A90 .

image:ai-superpod-a90-005.png["600.600"]

DGX SuperPOD offre i seguenti vantaggi:

* Prestazioni comprovate per carichi di lavoro AI/ML e HPC
* Stack hardware e software integrato, dalla gestione e monitoraggio dell'infrastruttura ai modelli e strumenti di deep learning predefiniti.
* Servizi dedicati, dall'installazione e gestione dell'infrastruttura al ridimensionamento dei carichi di lavoro e alla semplificazione dell'intelligenza artificiale di produzione.




== Panoramica della soluzione

Con l'adozione da parte delle organizzazioni di iniziative di intelligenza artificiale (IA) e apprendimento automatico (ML), la domanda di soluzioni infrastrutturali solide, scalabili ed efficienti non è mai stata così elevata.  Al centro di queste iniziative c'è la sfida di gestire e addestrare modelli di intelligenza artificiale sempre più complessi, garantendo al contempo la sicurezza dei dati, l'accessibilità e l'ottimizzazione delle risorse. 

Questa soluzione offre i seguenti vantaggi chiave:

* *Scalabilità*
* *Gestione e accesso ai dati*
* *Sicurezza*




=== Tecnologia delle soluzioni

NVIDIA DGX SuperPOD include i server, la rete e lo storage necessari per garantire prestazioni comprovate per carichi di lavoro di intelligenza artificiale impegnativi.  I sistemi NVIDIA DGX™ H200 e B200 offrono una potenza di elaborazione di livello mondiale, mentre gli switch di rete NVIDIA Quantum InfiniBand e Spectrum™ Ethernet garantiscono una latenza estremamente bassa e prestazioni di rete leader del settore.  Grazie all'aggiunta delle funzionalità di gestione dei dati e delle prestazioni leader del settore dello storage NetApp ONTAP , i clienti possono realizzare iniziative di intelligenza artificiale/apprendimento automatico più rapidamente e con meno migrazione dei dati e spese amministrative.  Per maggiori informazioni sui componenti specifici di questa soluzione, fare riferimento ahttps://www.netapp.com/pdf.html?item=/media/125003-nva-1175-design-superpod-a90.pdf["GUIDA ALLA PROGETTAZIONE NVA-1175"] E https://docs.nvidia.com/dgx-superpod/reference-architecture-scalable-infrastructure-b200/latest/index.html["+++ Architettura di riferimento NVIDIA DGX SuperPOD +++"] documentazione.



=== Riepilogo del caso d'uso

NVIDIA DGX SuperPOD è progettato per soddisfare i requisiti di prestazioni e scalabilità dei carichi di lavoro più impegnativi.

Questa soluzione si applica ai seguenti casi d'uso:

* Apprendimento automatico su larga scala mediante strumenti di analisi tradizionali.
* Formazione di modelli di intelligenza artificiale per modelli linguistici di grandi dimensioni, classificazione di immagini/visione artificiale, rilevamento di frodi e innumerevoli altri casi d'uso.
* Calcolo ad alte prestazioni, come analisi sismica, fluidodinamica computazionale e visualizzazione su larga scala.




== Requisiti tecnologici

DGX SuperPOD si basa sul concetto di unità scalabile (SU) che include tutti i componenti necessari per fornire la connettività e le prestazioni richieste ed eliminare eventuali colli di bottiglia nell'infrastruttura.  I clienti possono iniziare con una o più SU e aggiungerne altre in base alle proprie esigenze.  Per maggiori informazioni si prega di fare riferimento al https://docs.nvidia.com/dgx-superpod/reference-architecture-scalable-infrastructure-b200/latest/index.html["+++ Architettura di riferimento NVIDIA DGX SuperPOD +++"] .  Questo documento descrive i componenti di archiviazione e la configurazione per una singola SU.



=== Requisiti hardware

Nella tabella 1 sono elencati i componenti hardware necessari per implementare i componenti di archiviazione per 1SU.  Per informazioni specifiche sulle parti e sulle quantità per 1-4 unità scalabili, fare riferimento all'Appendice A.

Tabella 1) Requisiti hardware.

[cols="50%,50%"]
|===
| Hardware | Quantità 


| Sistema di archiviazione NetApp AFF A90 | 4 


| Switch di interconnessione del cluster di storage NetApp | 2 


| NVIDIA 800 GB -> 4 cavi splitter da 200 GB | 12 
|===


=== Requisiti software

Nella Tabella 2 sono elencati i componenti software minimi e le versioni necessarie per integrare il sistema di archiviazione AFF A90 con DGX SuperPOD.  DGX SuperPOD comprende anche altri componenti software non elencati qui.  Si prega di fare riferimento alhttps://docs.nvidia.com/dgx-superpod/release-notes/latest/10-24-11.html["+++Note di rilascio DGX SuperPOD+++"] per maggiori dettagli.

Tabella 2) Requisiti software.

[cols="50%,50%"]
|===
| Software | Versione 


| NetApp ONTAP | 9.16.1 o superiore 


| NVIDIA BaseCommand Manager | 10.24.11 o superiore 


| Sistema operativo NVIDIA DGX | 6.3.1 o superiore 


| Driver NVIDIA OFED | MLNX_OFED_LINUX-23.10.3.2.0 LTS o superiore 


| Sistema operativo NVIDIA Cumulus | 5.10 o superiore 
|===


== Procedure di distribuzione

L'integrazione dello storage NetApp ONTAP con DGX SuperPOD comporta le seguenti attività:

* Configurazione di rete per sistemi di storage NetApp AFF A90 con RoCE
* Installazione e configurazione del sistema di archiviazione
* Configurazione del client DGX con NVIDIA Base Command™ Manager




=== Installazione e configurazione del sistema di archiviazione



==== Preparazione del sito e installazione di base

La preparazione del sito e l'installazione di base del cluster di storage AFF A90 saranno eseguite da NetApp Professional Services per tutte le distribuzioni DGX SuperPOD come parte del servizio di distribuzione standard.  NetApp PS confermerà che le condizioni del sito sono idonee per l'installazione e installerà l'hardware nei rack designati.  Collegheranno inoltre le connessioni di rete OOB e completeranno la configurazione di base del cluster utilizzando le informazioni di rete fornite dal cliente.  Appendice A – Distinta base e quote dei rack include le quote standard dei rack come riferimento.  Per maggiori informazioni sull'installazione dell'A90 fare riferimento al https://docs.netapp.com/us-en/ontap-systems/a70-90/install-overview.html["+++ Documentazione di installazione hardware AFF A90 +++"] .

Una volta completata la distribuzione standard, NetApp PS completerà la configurazione avanzata della soluzione di storage utilizzando le procedure riportate di seguito, inclusa l'integrazione con Base Command Manager per la connettività e l'ottimizzazione del client.



==== Cablaggio del sistema di storage al fabric di storage DGX SuperPOD

Il sistema di storage AFF A90 è collegato agli switch leaf dello storage fabric tramite quattro porte Ethernet da 200 Gb per controller, con due connessioni per ogni switch.  Le porte dello switch da 800 Gb sugli switch NVIDIA Spectrum SN5600 sono suddivise in 4 porte da 200 Gb utilizzando le configurazioni DAC o splitter ottici appropriate elencate nell'Appendice A. Le singole porte di ogni porta dello switch sono distribuite sul controller di archiviazione per eliminare singoli punti di errore.  La figura 2 sottostante mostra il cablaggio per le connessioni dello storage fabric:

Figura 2) Cablaggio della rete di archiviazione.

image:ai-superpod-a90-006.png["600.600"]



==== Cablaggio del sistema di storage alla rete in-band DGX SuperPOD

NetApp ONTAP include funzionalità multi-tenancy leader del settore che gli consentono di funzionare sia come sistema di storage ad alte prestazioni nell'architettura DGX SuperPOD, sia di supportare directory home, condivisioni di file di gruppo e artefatti del cluster Base Command Manager.  Per l'utilizzo sulla rete in-band, ciascun controller AFF A90 è collegato agli switch di rete in-band con una connessione Ethernet da 200 Gb per controller e le porte sono configurate in una configurazione LACP MLAG.  La figura 3 sottostante mostra il cablaggio del sistema di storage alle reti in-band e OOB.

Figura 3) Cablaggio di rete in-band e OOB.

image:ai-superpod-a90-007.png["600.600"]



==== Configurare ONTAP per DGX SuperPOD

Questa soluzione sfrutta più Storage Virtual Machine (SVM) per ospitare volumi sia per l'accesso allo storage ad alte prestazioni sia per le directory home degli utenti e altri artefatti del cluster su una SVM di gestione.  Ogni SVM è configurato con interfacce di rete sulle reti di archiviazione o in banda e volumi FlexGroup per l'archiviazione dei dati.  Per garantire le prestazioni della Data SVM viene implementata una policy QoS di archiviazione.  Per ulteriori informazioni su FlexGroups, Storage Virtual Machines e funzionalità ONTAP QoS, fare riferimento a https://docs.netapp.com/us-en/ontap/index.html["+++ Documentazione ONTAP +++"] .



===== Configurare l'archiviazione di base



====== Configurare un singolo aggregato su ciascun controller

[source, cli]
----
aggr create -node <node> -aggregate <node>_data01 -diskcount <47> -maxraidsize 24
----
Ripetere i passaggi precedenti per ciascun nodo del cluster.



====== Configurare ifgrps su ciascun controller per la rete in banda

[source, cli]
----
net port ifgrp create -node <node> -ifgrp a1a -mode multimode
-distr-function port

net port ifgrp add-port -node <node> -ifgrp a1a -ports
<node>:e2a,<node>:e2b
----
Ripetere i passaggi precedenti per ciascun nodo del cluster.



====== Configurare le porte fisiche per RoCE

L'abilitazione di NFS su RDMA richiede una configurazione per garantire che il traffico di rete sia etichettato in modo appropriato sia sul client che sul server e che venga quindi gestito in modo appropriato dalla rete tramite RDMA su Converged Ethernet (RoCE).  Ciò include la configurazione del Priority Flow Control (PFC) e la configurazione della coda PFC CoS da utilizzare.  NetApp ONTAP configura automaticamente anche il codice DSCP 26 per allinearlo alla configurazione QoS della rete quando vengono eseguiti i comandi seguenti.

[source, cli]
----
network port modify -node * -port e6* -flowcontrol-admin pfc
-pfc-queues-admin 3

network port modify -node * -port e11* -flowcontrol-admin pfc
-pfc-queues-admin 3
----


====== Crea domini di trasmissione

[source, cli]
----
broadcast-domain create -broadcast-domain in-band -mtu 9000 -ports
ntapa90_spod-01:a1a,ntapa90_spod-02:a1a,ntapa90_spod-03:a1a,ntapa90_spod-04:a1a,ntapa90_spod-05:a1a,
ntapa90_spod-06:a1a,ntapa90_spod-07:a1a,ntapa90_spod-08:a1a

broadcast-domain create -broadcast-domain vlan401 -mtu 9000 -ports
ntapa90_spod-01:e6a,ntapa90_spod-01:e6b,ntapa90_spod-02:e6a,ntapa90_spod-02:e6b,ntapa90_spod-03:e6a,ntapa90_spod-03:e6b,ntapa90_spod-04:e6a,ntapa90_spod-04:e6b,ntapa90_spod-05:e6a,ntapa90_spod-05:e6b,ntapa90_spod-06:e6a,ntapa90_spod-06:e6b,ntapa90_spod-07:e6a,ntapa90_spod-07:e6b,ntapa90_spod-08:e6a,ntapa90_spod-08:e6b

broadcast-domain create -broadcast-domain vlan402 -mtu 9000 -ports
ntapa90_spod-01:e11a,ntapa90_spod-01:e11b,ntapa90_spod-02:e11a,ntapa90_spod-02:e11b,ntapa90_spod-03:e11a,ntapa90_spod-03:e11b,ntapa90_spod-04:e11a,ntapa90_spod-04:e11b,ntapa90_spod-05:e11a,ntapa90_spod-05:e11b,ntapa90_spod-06:e11a,ntapa90_spod-06:e11b,ntapa90_spod-07:e11a,ntapa90_spod-07:e11b,ntapa90_spod-08:e11a,ntapa90_spod-08:e11b

----


===== Crea SVM di gestione



====== Creare e configurare Management SVM

[source, cli]
----
vserver create -vserver spod_mgmt

vserver modify -vserver spod_mgmt -aggr-list
ntapa90_spod-01_data01,ntapa90_spod-02_data01,
ntapa90_spod-03_data01,ntapa90_spod-04_data01,
ntapa90_spod-05_data01,ntapa90_spod-06_data01,
ntapa90_spod-07_data01,ntapa90_spod-08_data01
----


====== Configurare il servizio NFS su Management SVM

[source, cli]
----
nfs create -vserver spod_mgmt -v3 enabled -v4.1 enabled -v4.1-pnfs
enabled -tcp-max-xfer-size 262144 -v4.1-trunking enabled

set advanced

nfs modify -vserver spod_mgmt -v3-64bit-identifiers enabled
-v4.x-session-num-slots 1024
----


====== Creare subnet IP per interfacce di rete in banda

[source, cli]
----
network subnet create -subnet-name inband -broadcast-domain in-band
-subnet xxx.xxx.xxx.0/24 -gateway xxx.xxx.xxx.x -ip-ranges
xxx.xxx.xxx.xx-xxx.xxx.xxx.xxx
----
*Nota:* le informazioni sulla subnet IP devono essere fornite dal cliente al momento dell'implementazione per l'integrazione nelle reti esistenti del cliente.



====== Crea interfacce di rete su ciascun nodo per SVM in banda

[source, cli]
----
net int create -vserver spod_mgmt -lif inband_lif1 -home-node
ntapa90_spod-01 -home-port a1a -subnet_name inband
----
Ripetere i passaggi precedenti per ciascun nodo del cluster.



====== Crea volumi FlexGroup per Management SVM

[source, cli]
----
vol create -vserver spod_mgmt -volume home -size 10T -auto-provision-as
flexgroup -junction-path /home

vol create -vserver spod_mgmt -volume cm -size 10T -auto-provision-as
flexgroup -junction-path /cm

----


====== Crea una policy di esportazione per Management SVM

[source, cli]
----
export-policy rule create -vserver spod_mgmt -policy default
-client-match XXX.XXX.XXX.XXX -rorule sys -rwrule sys -superuser sys
----
*Nota:* le informazioni sulla subnet IP devono essere fornite dal cliente al momento dell'implementazione per l'integrazione nelle reti esistenti del cliente.



===== Crea dati SVM



====== Creare e configurare Data SVM

[source, cli]
----
vserver create -vserver spod_data
vserver modify -vserver spod_data -aggr-list
ntapa90_spod-01_data01,ntapa90_spod-02_data01,
ntapa90_spod-03_data01,ntapa90_spod-04_data01,
ntapa90_spod-05_data01,ntapa90_spod-06_data01,
ntapa90_spod-07_data01,ntapa90_spod-08_data01
----


====== Configurare il servizio NFS su Data SVM con RDMA abilitato

[source, cli]
----
nfs create -vserver spod_data -v3 enabled -v4.1 enabled -v4.1-pnfs
enabled -tcp-max-xfer-size 262144 -v4.1-trunking enabled -rdma enabled

set advanced

nfs modify -vserver spod_data -v3-64bit-identifiers enabled
-v4.x-session-num-slots 1024
----


====== Creare subnet IP per le interfacce di rete Data SVM

[source, cli]
----
network subnet create -subnet-name vlan401 -broadcast-domain vlan401
-subnet 100.127.124.0/24 -ip-ranges 100.127.124.4-100.127.124.254

network subnet create -subnet-name vlan402 -broadcast-domain vlan402
-subnet 100.127.252.0/24 -ip-ranges 100.127.252.4-100.127.252.254
----


====== Crea interfacce di rete su ciascun nodo per Data SVM

[source, cli]
----
net int create -vserver spod_data -lif data_lif1 -home-node
ntapa90_spod-01 -home-port e6a -subnet_name vlan401 -failover-policy
sfo-partner-only

net int create -vserver spod_data -lif data_lif2 -home-node
ntapa90_spod-01 -home-port e6b -subnet_name vlan401

net int create -vserver spod_data -lif data_lif3 -home-node
ntapa90_spod-01 -home-port e11a -subnet_name vlan402

net int create -vserver spod_data -lif data_lif4 -home-node
ntapa90_spod-01 -home-port e11b -subnet_name vlan402

----
Ripetere i passaggi precedenti per ciascun nodo del cluster.



====== Configurare le interfacce di rete Data SVM per RDMA

[source, cli]
----
net int modify -vserver spod_data -lif * -rdma-protocols roce
----


====== Crea una policy di esportazione sui dati SVM

[source, cli]
----
export-policy rule create -vserver spod_data -policy default
-client-match 100.127.0.0/16 -rorule sys -rwrule sys -superuser sys
----


====== Crea percorsi statici su dati SVM

[source, cli]
----
route add -vserver spod_data -destination 100.127.0.0/17 -gateway
100.127.124.1 -metric 20

route add -vserver spod_data -destination 100.127.0.0/17 -gateway
100.127.252.1 -metric 30

route add -vserver spod_data -destination 100.127.128.0/17 -gateway
100.127.252.1 -metric 20

route add -vserver spod_data -destination 100.127.128.0/17 -gateway
100.127.124.1 -metric 30
----


====== Crea un volume FlexGroup con GDD per Data SVM

La distribuzione granulare dei dati (GDD) consente di distribuire file di dati di grandi dimensioni su più volumi e controller costituenti FlexGroup per garantire le massime prestazioni per carichi di lavoro a file singolo.  NetApp consiglia di abilitare GDD sui volumi di dati per tutte le distribuzioni DGX SuperPOD.

[source, cli]
----
set adv

vol create -vserver spod-data -volume spod_data -size 100T -aggr-list
ntapa90_spod-01_data01,ntapa90_spod-02_data01,
ntapa90_spod-03_data01,ntapa90_spod-04_data01,
ntapa90_spod-05_data01,ntapa90_spod-06_data01,
ntapa90_spod-07_data01,ntapa90_spod-08_data01 -aggr-multiplier 16
-granular-data advanced -junction-path /spod_data  
----


====== Disabilita l'efficienza di archiviazione per il volume di dati primario

efficienza del volume disattivata -vserver spod_data -volume spod_data



====== Creare una policy minima QoS per la SVM dei dati

[source, cli]
----
qos policy-group create -policy-group spod_qos -vserver spod_data
-min-throughput 62GB/s -is-shared true
----


====== Applica la politica QoS per i dati SVM

[source, cli]
----
Volume modify -vserver spod_data -volume spod_data -qos-policy-group
spod_qos
----


=== Configurazione del server DGX con NVIDIA Base Command Manager

Per preparare i client DGX all'utilizzo del sistema di archiviazione AFF A90 , completare le seguenti attività.  Questo processo presuppone che le interfacce di rete e i percorsi statici per la struttura di archiviazione siano già stati configurati sui nodi del sistema DGX.  Le seguenti attività saranno completate da NetApp Professional Services come parte del processo di configurazione avanzata.



==== Configurare l'immagine del server DGX con i parametri del kernel richiesti e altre impostazioni

NetApp ONTAP utilizza protocolli NFS standard del settore e non richiede l'installazione di alcun software aggiuntivo sui sistemi DGX.  Per garantire prestazioni ottimali dai sistemi client sono necessarie diverse modifiche all'immagine del sistema DGX.  Entrambi i passaggi seguenti vengono eseguiti dopo essere entrati nella modalità chroot dell'immagine BCM utilizzando il comando seguente:

[source, cli]
----
cm-chroot-sw-img /cm/images/<image>
----


===== Configurare le impostazioni della memoria virtuale di sistema in /etc/sysctl.conf

La configurazione predefinita del sistema Linux prevede impostazioni di memoria virtuale che potrebbero non garantire necessariamente prestazioni ottimali.  Nel caso dei sistemi DGX B200 con 2 TB di RAM, le impostazioni predefinite consentono 40 GB di spazio buffer, il che crea modelli di I/O incoerenti e consente al client di sovraccaricare il sistema di archiviazione durante lo svuotamento del buffer.  Le impostazioni seguenti limitano lo spazio del buffer del client a 5 GB e forzano lo svuotamento più spesso per creare un flusso I/O coerente che non sovraccarichi il sistema di archiviazione.

Dopo essere entrati nella modalità chroot dell'immagine, modificate il file /etc/sysctl.s/90-cm-sysctl.conf e aggiungete le seguenti righe:

[source, cli]
----
vm.dirty_ratio=0 #controls max host RAM used for buffering as a
percentage of total RAM, when this limit is reached all applications
must flush buffers to continue

vm.dirty_background_ratio=0 #controls low-watermark threshold to start
background flushing as a percentage of total RAM

vm.dirty_bytes=5368709120 #controls max host RAM used for buffering as
an absolute value (note _ratio above only accepts integers and the value
we need is <1% of total RAM (2TB))

vm.dirty_background_bytes=2147483648 #controls low-watermark threshold
to start background flushing as an absolute value

vm.dirty_expire_centisecs = 300 #controls how long data remains in
buffer pages before being marked dirty

vm.dirty_writeback_centisecs = 100 #controls how frequently the flushing
process wakes up to flush dirty buffers
----
Salvare e chiudere il file /etc/sysctl.conf.



===== Configura altre impostazioni di sistema con uno script che viene eseguito dopo il riavvio

Alcune impostazioni richiedono che il sistema operativo sia completamente online per essere eseguite e non sono persistenti dopo un riavvio.  Per eseguire queste impostazioni in un ambiente Base Command Manager, creare un file /root/ntap_dgx_config.sh e immettere le seguenti righe:

[source, cli]
----
#!/bin/bash

##The commands below are platform-specific based.

##For H100/H200 systems use the following variables

## NIC1_ethname= enp170s0f0np0

## NIC1_pciname=aa:00.0

## NCI1_mlxname=mlx5_7

## NIC1_ethname= enp41s0f0np0

## NIC1_pciname=29:00.0

## NCI1_mlxname=mlx5_1

##For B200 systems use the following variables

NIC1_ethname=enp170s0f0np0

NIC1_pciname=aa:00.0

NCI1_mlxname=mlx5_11

NIC2_ethname=enp41s0f0np0

NIC2_pciname=29:00.0

NCI2_mlxname=mlx5_5

mstconfig -y -d $\{NIC1_pciname} set ADVANCED_PCI_SETTINGS=1
NUM_OF_VFS=0

mstconfig -y -d $\{NIC2_pciname} set ADVANCED_PCI_SETTINGS=1
NUM_OF_VFS=0

setpci -s $\{NIC1_pciname} 68.W=5957

setpci -s $\{NIC2_pciname} 68.W=5957

ethtool -G $\{NIC1_ethname} rx 8192 tx 8192

ethtool -G $\{NIC2_ethname} rx 8192 tx 8192

mlnx_qos -i $\{NIC1_ethname} --pfc 0,0,0,1,0,0,0,0 --trust=dscp

mlnx_qos -i $\{NIC2_ethname} --pfc 0,0,0,1,0,0,0,0 --trust=dscp

echo 106 > /sys/class/infiniband/$\{NIC1_mlxname}/tc/1/traffic_class

echo 106 > /sys/class/infiniband/$\{NIC2_mlxname}/tc/1/traffic_class
----
*Salvare e chiudere il file.  Modificare i permessi sul file in modo che sia eseguibile:*

[source, cli]
----
chmod 755 /root/ntap_dgx_config.sh
----
Crea un cron job che venga eseguito da root all'avvio modificando la seguente riga:

[source, cli]
----
@reboot /root/ntap_dgx_config.sh
----
Vedere il file crontab di esempio qui sotto:

[source, cli]
----
# Edit this file to introduce tasks to be run by cron.

#

# Each task to run has to be defined through a single line

# indicating with different fields when the task will be run

# and what command to run for the task

#

# To define the time you can provide concrete values for

# minute (m), hour (h), day of month (dom), month (mon),

# and day of week (dow) or use '*' in these fields (for 'any').

#

# Notice that tasks will be started based on the cron's system

# daemon's notion of time and timezones.

#

# Output of the crontab jobs (including errors) is sent through

# email to the user the crontab file belongs to (unless redirected).

#

# For example, you can run a backup of all your user accounts

# at 5 a.m every week with:

# 0 5 * * 1 tar -zcf /var/backups/home.tgz /home/

#

# For more information see the manual pages of crontab(5) and cron(8)

#

# m h dom mon dow command

@reboot /home/ntap_dgx_config.sh
----
Uscire dalla modalità chroot dell'immagine BCM immettendo exit o Ctrl-D.



==== Configurare la categoria DGX di BaseCommand Manager per i punti di montaggio del client

Per configurare i client DGX che montano il sistema di archiviazione AFF A90 , la categoria client BCM utilizzata dai sistemi DGX deve essere modificata per includere le informazioni e le opzioni pertinenti.  I passaggi seguenti descrivono come configurare il punto di montaggio NFS.

[source, cli]
----
cmsh

category ; use category <category>; fsmounts

add superpod

set device 100.127.124.4:/superpod

set mountpoint /mnt/superpod

set filesystem nfs

set mountoptions
vers=4.1,proto=rdma,max_connect=16,write=eager,rsize=262144,wsize=262144

commit
----


== Conclusione

NVIDIA DGX SuperPOD con i sistemi di storage NetApp * AFF A90 * rappresenta un significativo progresso nelle soluzioni infrastrutturali AI.  Affrontando le principali sfide legate a sicurezza, gestione dei dati, utilizzo delle risorse e scalabilità, consente alle organizzazioni di accelerare le proprie iniziative di intelligenza artificiale mantenendo al contempo efficienza operativa, protezione dei dati e collaborazione.  L'approccio integrato della soluzione elimina i comuni colli di bottiglia nelle pipeline di sviluppo dell'intelligenza artificiale, consentendo a data scientist e ingegneri di concentrarsi sull'innovazione anziché sulla gestione dell'infrastruttura.



== Dove trovare ulteriori informazioni

Per saperne di più sulle informazioni descritte nel presente documento, consultare i seguenti documenti e/o siti web:

* https://www.netapp.com/pdf.html?item=/media/125003-nva-1175-design-superpod-a90.pdf["Guida alla progettazione dei sistemi di storage NVA-1175 NVIDIA DGX SuperPOD con NetApp AFF A90"]
* https://docs.nvidia.com/dgx-superpod/reference-architecture-scalable-infrastructure-b200/latest/index.html["Architettura di riferimento NVIDIA DGX B200 SuperPOD"]
* https://docs.nvidia.com/dgx-superpod/reference-architecture/scalable-infrastructure-h200/latest/index.html["+++ Architettura di riferimento NVIDIA DGX H200 SuperPOD+++"]
* https://docs.nvidia.com/base-command-manager/index.html#product-manuals["+++ Software NVIDIA BaseCommand+++"]
* https://nvdam.widen.net/s/mmvbnpk8qk/networking-ethernet-switches-sn5000-datasheet-us["+++ Switch Ethernet NVIDIA Spectrum SN5600+++"]
* https://docs.nvidia.com/dgx-superpod/release-notes/latest/10-24-11.html["+++ Note sulla versione NVIDIA DGX SuperPOD +++"]
* https://docs.netapp.com/us-en/ontap-systems/a70-90/install-overview.html["+++ Installazione NetApp AFF A90 +++"]
* https://docs.netapp.com/us-en/netapp-solutions/ai/index.html["+++ Documentazione sulle soluzioni AI NetApp +++"]
* https://docs.netapp.com/us-en/ontap/index.html["+++ Software NetApp ONTAP +++"]
* https://docs.netapp.com/us-en/ontap-systems/aff-aseries/index.html["+++ NetApp installa e gestisce i sistemi di archiviazione AFF +++"]
* https://docs.netapp.com/us-en/ontap/nfs-rdma/index.html["NFS su RDMA"]
* https://www.netapp.com/media/19761-tr-4063.pdf["+++Cos'è pNFS?+++"](vecchio documento con ottime informazioni sul pNFS)




== Appendice A: Distinta base e quote dei rack



=== distinta base

Nella tabella 3 sono riportati il ​​numero di parte e le quantità dei componenti NetApp necessari per distribuire lo storage per una, due, tre e quattro unità scalabili.

Tabella 3) NetApp BOM per 1, 2, 3 e 4 SU.

[cols="20%,32%,12%,12%,12%,12%"]
|===
| Parte n. | Articolo | Quantità per 1 unità unitaria | Quantità per 2SU | Quantità per 3SU | Quantità per 4SU 


| AFF-A90A-100-C | Sistema di stoccaggio AFF A90 | 4 | 8 | 12 | 16 


| X4025A-2-A-C | Pacchetto unità 2x7,6 TB | 48 | 96 | 144 | 192 


| X50131A-C | Modulo IO, 2PT, 100/200/400 GbE | 24 | 48 | 96 | 128 


| X50130A-C | Modulo IO, 2PT, 100GbE | 16 | 32 | 48 | 64 


| X-02659-00 | Kit, 4 montanti, foro quadrato o rotondo, binario da 24"-32" | 4 | 8 | 12 | 16 


| X1558A-R6 | Cavo di alimentazione, in armadio, 48 pollici, + C13-C14, 10 A/250 V | 20 | 40 | 60 | 80 


| X190200-CS | Interruttore a grappolo, N9336C 36Pt PTSX10/25/40/100G | 2 | 4 | 6 | 8 


| X66211A-2 | Cavo, 100 GbE, QSFP28-QSFP28, Cu, 2 m | 16 | 32 | 48 | 64 


| X66211A-05 | Cavo, 100 GbE, QSFP28-QSFP28, Cu, 0,5 m | 4 | 8 | 12 | 16 


| X6561-R6 | Cavo, Ethernet, CAT6, RJ45, 5 m | 18 | 34 | 50 | 66 
|===
La tabella 4 mostra il numero di parte e la quantità di cavi NVIDIA necessari per collegare i sistemi di storage AFF A90 agli switch SN5600 nelle reti di storage ad alte prestazioni e in-band.

Tabella 4) Cavi NVIDIA necessari per collegare i sistemi di storage AFF A90 agli switch SN5600 nelle reti di storage ad alte prestazioni e in-band.

[cols="20%,32%,12%,12%,12%,12%"]
|===
| Parte n. | Articolo | Quantità per 1 unità unitaria | Quantità per 2SU | Quantità per 3SU | Quantità per 4SU 


| MCP7Y40-N003 | DAC 3m 26ga da 2x400G a 4x200G OSFP a 4xQSFP112 | 12 | 24 | 36 | 48 


| O |  |  |  |  |  


| MMS4X00-NS | Transceiver multimodale OSFP 2x400G 2xSR4 a doppia porta, doppio MPO-12/APC | 12 | 24 | 36 | 48 


| MFP7E20-N0XX | Splitter per fibre multimodali 400G-> 2x200G XX = 03, 05, 07, 10, 15, 20, 30, 40, 50) metri | 24 | 48 | 96 | 128 


| MMA1Z00-NS400 | Transceiver QSFP112 multimodale SR4 a porta singola 400G MPO-12/APC singolo | 48 | 96 | 144 | 192 
|===


=== Elevazioni del rack

Le figure 4-6 mostrano esempi di elevazioni dei rack per 1-4 SU.

Figura 4) Elevazioni dei rack per 1 SU e 2 SU.

image:ai-superpod-a90-008.png["600.600"]

Figura 5) Elevazioni dei rack per 3 SU.

image:ai-superpod-a90-009.png["600.600"]

Figura 6) Elevazioni dei rack per 4 SU.

image:ai-superpod-a90-010.png["600.600"]
